# **Efficient Multi-Tasking with Small Language Models (SLMs)**  

## **Overview**  
This project explores the efficiency of **Small Language Models (SLMs)** for multi-task learning, offering a lightweight and cost-effective alternative to **Large Language Models (LLMs)**. We fine-tuned **Microsoftâ€™s Phi-3.5-mini-instruct** to handle three key tasks:  

- **Reasoning** (using the ROPES dataset)  
- **Web Search Summarization** (Google-style retrieval)  
- **Mathematical Problem-Solving** (using the GSM8K dataset)  

Our approach optimizes model performance while maintaining **low computational costs**, making AI more accessible for real-time applications.  

## **Methodology**  
### ðŸš€ **Techniques Used**  
âœ… **Multi-task Fine-tuning** â†’ Improves knowledge transfer across different tasks  
âœ… **LoRA Adapters** â†’ Efficient parameter updates without full model retraining  
âœ… **Chain of Thought (CoT) Prompting** â†’ Enhances logical reasoning and step-by-step solutions  
âœ… **Gradient Clipping & Adaptive Learning Rate** â†’ Ensures stable and efficient training  

## **Results**  
ðŸ“Œ **Reasoning Task:** **75% accuracy**, outperforming Llama-3-1.3B by **3x**  
ðŸ“Œ **Web Search Summarization:** Summary generated in **80.9s**, achieving strong **ROUGE scores**  
ðŸ“Œ **Mathematical Problem-Solving:** **91.51% accuracy**, effectively handling complex calculations  

## **Key Takeaways**  
ðŸ”¹ **SLMs can match or outperform larger models on focused tasks**  
ðŸ”¹ **Efficient fine-tuning strategies reduce computational overhead**  
ðŸ”¹ **Potential for real-time AI applications in education, search, and coding assistance**  

## **Future Scope**  
ðŸš§ **Expanding the modelâ€™s multilingual capabilities**  
ðŸš§ **Enhancing real-time coding support & debugging assistance**  
ðŸš§ **Optimizing for domain-specific tasks & personalization**  

## **Contributions & Acknowledgments**  
ðŸ™Œ This project builds on Microsoftâ€™s **Phi-3.5-mini-instruct** and integrates best practices from recent AI research on multi-task learning. Contributions and suggestions are welcome!  
